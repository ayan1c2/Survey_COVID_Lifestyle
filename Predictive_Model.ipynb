{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e193128",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/principal-components-analysis-for-dimensionality-reduction-in-python/\n",
    "#https://towardsdatascience.com/principal-component-analysis-for-dimensionality-reduction-115a3d157bad\n",
    "#https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
    "#PCA for dimensionality-reduction\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, ComplementNB\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62859e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dataset\n",
    "def get_dataset(df):\n",
    "\tX, y = df.iloc[:, 0:-1], df.iloc[:, -1]\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfcd5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of models to evaluate\n",
    "def get_models(features, model):\n",
    "\tmodels = dict()\n",
    "\tfor i in range(1,features):\n",
    "\t\tsteps = [('pca', PCA(n_components=i)), ('m', model)]\n",
    "\t\tmodels[str(i)] = Pipeline(steps=steps)\n",
    "\treturn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faefffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\treturn scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cae93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Survey_Predictive_Analysis.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f8ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef873eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_dataset(df)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff3493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X.shape[1]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8cd4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model LogisticRegression\n",
    "model = LogisticRegression()\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models(features, model)\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model in models.items():\n",
    "\tscores, f1, recall, precision = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)    \n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.2f (%.2f)' % (name, mean(scores), std(scores)))\n",
    "    \n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.xticks(rotation=45)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a095e6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models(features, model)\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.2f (%.2f)' % (name, mean(scores), std(scores)))\n",
    "    \n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.xticks(rotation=45)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883f75ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(criterion=\"gini\")\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models(features, model)\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.2f (%.2f)' % (name, mean(scores), std(scores)))\n",
    "    \n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.xticks(rotation=45)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f3e11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model SVC\n",
    "model = SVC(kernel='linear')\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models(features, model)\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.2f (%.2f)' % (name, mean(scores), std(scores)))\n",
    "    \n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.xticks(rotation=45)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee383bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model SVC\n",
    "model = SVC(kernel='rbf')\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models(features, model)\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.2f (%.2f)' % (name, mean(scores), std(scores)))\n",
    "    \n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.xticks(rotation=45)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3da7401",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models(features, model)\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.2f (%.2f)' % (name, mean(scores), std(scores)))\n",
    "    \n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.xticks(rotation=45)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3d86e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BernoulliNB()\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models(features, model)\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.2f (%.2f)' % (name, mean(scores), std(scores)))\n",
    "    \n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.xticks(rotation=45)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a1893",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators = 50)\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models(features, model)\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.2f (%.2f)' % (name, mean(scores), std(scores)))\n",
    "    \n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.xticks(rotation=45)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b2cdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models(features, model)\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.2f (%.2f)' % (name, mean(scores), std(scores)))\n",
    "    \n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.xticks(rotation=45)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322826d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators = 150)\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models(features, model)\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.2f (%.2f)' % (name, mean(scores), std(scores)))\n",
    "    \n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.xticks(rotation=45)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464a4f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors = 2)  \n",
    "# get the models to evaluate\n",
    "models = get_models(features, model)\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.2f (%.2f)' % (name, mean(scores), std(scores)))\n",
    "    \n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.xticks(rotation=45)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a039ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors = 4)  \n",
    "# get the models to evaluate\n",
    "models = get_models(features, model)\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.2f (%.2f)' % (name, mean(scores), std(scores)))\n",
    "    \n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.xticks(rotation=45)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1c677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors = 6)  \n",
    "# get the models to evaluate\n",
    "models = get_models(features, model)\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.2f (%.2f)' % (name, mean(scores), std(scores)))\n",
    "    \n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.xticks(rotation=45)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5d8742",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearDiscriminantAnalysis() \n",
    "# get the models to evaluate\n",
    "models = get_models(features, model)\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.2f (%.2f)' % (name, mean(scores), std(scores)))\n",
    "    \n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.xticks(rotation=45)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fc02a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaggingClassifier(base_estimator = DecisionTreeClassifier(), n_estimators = 200, random_state = 7)\n",
    "# get the models to evaluate\n",
    "models = get_models(features, model)\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.2f (%.2f)' % (name, mean(scores), std(scores)))\n",
    "    \n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.xticks(rotation=45)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14bebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaggingClassifier(base_estimator = DecisionTreeClassifier(), n_estimators = 100, random_state = 7)\n",
    "# get the models to evaluate\n",
    "models = get_models(features, model)\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.2f (%.2f)' % (name, mean(scores), std(scores)))\n",
    "    \n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.xticks(rotation=45)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6d9fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(n_estimators = 100, random_state = 7)\n",
    "# get the models to evaluate\n",
    "models = get_models(features, model)\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.2f (%.2f)' % (name, mean(scores), std(scores)))\n",
    "    \n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.xticks(rotation=45)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996db01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(n_estimators = 200, random_state = 7)\n",
    "# get the models to evaluate\n",
    "models = get_models(features, model)\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.2f (%.2f)' % (name, mean(scores), std(scores)))\n",
    "    \n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.xticks(rotation=45)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7f96d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AdaBoostClassifier(n_estimators = 100, random_state = 7)\n",
    "# get the models to evaluate\n",
    "models = get_models(features, model)\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.2f (%.2f)' % (name, mean(scores), std(scores)))\n",
    "    \n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.xticks(rotation=45)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf8739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AdaBoostClassifier(n_estimators = 200, random_state = 7)\n",
    "# get the models to evaluate\n",
    "models = get_models(features, model)\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.2f (%.2f)' % (name, mean(scores), std(scores)))\n",
    "    \n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.xticks(rotation=45)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff428b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier(n_estimators = 100, random_state = 7)\n",
    "# get the models to evaluate\n",
    "models = get_models(features, model)\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.2f (%.2f)' % (name, mean(scores), std(scores)))\n",
    "    \n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.xticks(rotation=45)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985d687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier(n_estimators = 200, random_state = 7)\n",
    "# get the models to evaluate\n",
    "models = get_models(features, model)\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.2f (%.2f)' % (name, mean(scores), std(scores)))\n",
    "    \n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.xticks(rotation=45)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239002ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_tune(X,y,fold,model):    \n",
    "    # Create regularization penalty space\n",
    "    penalty = ['l1', 'l2']\n",
    "\n",
    "    # Create regularization hyperparameter space\n",
    "    C = np.logspace(0, 0.0001, 1)\n",
    "\n",
    "    # Create hyperparameter options\n",
    "    hyperparameters = dict(C=C, penalty=penalty)\n",
    "    \n",
    "    # Create grid search using 5-fold cross validation\n",
    "    clf = GridSearchCV(model, hyperparameters, cv=fold, verbose=0)\n",
    "    \n",
    "    # Fit grid search\n",
    "    best_model = clf.fit(X, y)\n",
    "    \n",
    "    # View best hyperparameters\n",
    "    print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "    print('Best C:', best_model.best_estimator_.get_params()['C'])\n",
    "    \n",
    "    # Predict target vector\n",
    "    best_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d13864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "LR_tune(X,y,10,LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c691674",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation Curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "param_range = np.logspace(-6, -1, 5)\n",
    "train_scores, test_scores = validation_curve(\n",
    "    SVC(kernel = 'rbf'),\n",
    "    X,\n",
    "    y,\n",
    "    param_name=\"gamma\",\n",
    "    param_range=param_range,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=2,\n",
    ")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title(\"Validation Curve with SVM\")\n",
    "plt.xlabel(r\"$\\gamma$\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 2\n",
    "plt.semilogx(\n",
    "    param_range, train_scores_mean, label=\"Training score\", color=\"darkorange\", lw=lw\n",
    ")\n",
    "plt.fill_between(\n",
    "    param_range,\n",
    "    train_scores_mean - train_scores_std,\n",
    "    train_scores_mean + train_scores_std,\n",
    "    alpha=0.2,\n",
    "    color=\"darkorange\",\n",
    "    lw=lw,\n",
    ")\n",
    "plt.semilogx(\n",
    "    param_range, test_scores_mean, label=\"Cross-validation score\", color=\"navy\", lw=lw\n",
    ")\n",
    "plt.fill_between(\n",
    "    param_range,\n",
    "    test_scores_mean - test_scores_std,\n",
    "    test_scores_mean + test_scores_std,\n",
    "    alpha=0.2,\n",
    "    color=\"navy\",\n",
    "    lw=lw,\n",
    ")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aad37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning curve\n",
    "lr = LogisticRegression()\n",
    "svc = SVC(kernel=\"rbf\", gamma=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9592c8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve, ShuffleSplit\n",
    "\n",
    "common_params = {\n",
    "    \"X\": X,\n",
    "    \"y\": y,\n",
    "    \"train_sizes\": np.linspace(0.1, 1.0, 5),\n",
    "    \"cv\": ShuffleSplit(n_splits=50, test_size=0.2, random_state=0),\n",
    "    \"n_jobs\": 4,\n",
    "    \"return_times\": True,\n",
    "}\n",
    "\n",
    "train_sizes, _, test_scores_lr, fit_times_lr, score_times_lr = learning_curve(\n",
    "    lr, **common_params\n",
    ")\n",
    "train_sizes, _, test_scores_svm, fit_times_svm, score_times_svm = learning_curve(\n",
    "    svc, **common_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807a7cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(16, 12), sharex=True)\n",
    "\n",
    "for ax_idx, (fit_times, score_times, estimator) in enumerate(\n",
    "    zip(\n",
    "        [fit_times_lr, fit_times_svm],\n",
    "        [score_times_lr, score_times_svm],\n",
    "        [lr, svc],\n",
    "    )\n",
    "):\n",
    "    # scalability regarding the fit time\n",
    "    ax[0, ax_idx].plot(train_sizes, fit_times.mean(axis=1), \"o-\")\n",
    "    ax[0, ax_idx].fill_between(\n",
    "        train_sizes,\n",
    "        fit_times.mean(axis=1) - fit_times.std(axis=1),\n",
    "        fit_times.mean(axis=1) + fit_times.std(axis=1),\n",
    "        alpha=0.3,\n",
    "    )\n",
    "    ax[0, ax_idx].set_ylabel(\"Fit time (s)\")\n",
    "    ax[0, ax_idx].set_title(\n",
    "        f\"Scalability of the {estimator.__class__.__name__} classifier\"\n",
    "    )\n",
    "\n",
    "    # scalability regarding the score time\n",
    "    ax[1, ax_idx].plot(train_sizes, score_times.mean(axis=1), \"o-\")\n",
    "    ax[1, ax_idx].fill_between(\n",
    "        train_sizes,\n",
    "        score_times.mean(axis=1) - score_times.std(axis=1),\n",
    "        score_times.mean(axis=1) + score_times.std(axis=1),\n",
    "        alpha=0.3,\n",
    "    )\n",
    "    ax[1, ax_idx].set_ylabel(\"Score time (s)\")\n",
    "    ax[1, ax_idx].set_xlabel(\"Number of training samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b83a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "\n",
    "y_predict = svc.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_predict))\n",
    "confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aa6371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lists to collect scores\n",
    "train_scores, test_scores = list(), list()\n",
    "\n",
    "# define the tree depths to evaluate\n",
    "values = [i for i in range(1, 50)]\n",
    "# evaluate a decision tree for each depth\n",
    "\n",
    "for i in values:\n",
    "\t# configure the model\n",
    "\tmodel = ExtraTreesClassifier(max_depth=i)\n",
    "\t# fit model on the training dataset\n",
    "\tmodel.fit(X_train, y_train)\n",
    "\t# evaluate on the train dataset\n",
    "\ttrain_yhat = model.predict(X_train)\n",
    "\ttrain_acc = accuracy_score(y_train, train_yhat)\n",
    "\ttrain_scores.append(train_acc)\n",
    "\t# evaluate on the test dataset\n",
    "\ttest_yhat = model.predict(X_test)\n",
    "\ttest_acc = accuracy_score(y_test, test_yhat)\n",
    "\ttest_scores.append(test_acc)\n",
    "\t# summarize progress\n",
    "\tprint('>%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc))\n",
    "    \n",
    "# plot of train and test scores vs tree depth\n",
    "plt.plot(values, train_scores, '-o', label='Train')\n",
    "plt.plot(values, test_scores, '-o', label='Test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6954c2f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17852\\855181586.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c660b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "feature_names = df.feature_names\n",
    "\n",
    "# Create PCA and SVC pipeline\n",
    "pipe = make_pipeline(PCA(n_components=31), SVC(kernel='linear'))\n",
    "\n",
    "# Fit the pipeline\n",
    "pipe.fit(X, y)\n",
    "\n",
    "# Create LIME explainer\n",
    "explainer = LimeTabularExplainer(X, feature_names=feature_names, class_names=iris.target_names)\n",
    "\n",
    "# Select a sample for explanation\n",
    "sample_idx = 0\n",
    "sample = X[sample_idx].reshape(1, -1)\n",
    "\n",
    "# Explain the sample prediction\n",
    "exp = explainer.explain_instance(sample[0], pipe.predict_proba)\n",
    "\n",
    "# Show explanation\n",
    "exp.show_in_notebook()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
